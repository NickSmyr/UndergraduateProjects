<!DOCTYPE html>
<html lang="el">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Deep Learning - A quick introduction</title>
        <link rel="stylesheet" href="./stylesheet.css">
    </head>
    <body>
        <header>
            <h1>Deep Learning - A quick introduction</h1>
            <nav>
                <ul>
                    <li>
                        <a href="index.html">Θεωρία</a>
                    </li>
                    <li>
                        <a href="tools.html">Εργαλεία</a>
                    </li>
                    <li>
                        <a href="future.html">Μέλλον</a>
                    </li>
                </ul>
            </nav>
        </header>
        
        <main>
            
            <article>
                <h1>Μέλλον</h1>
                <section>
                    <h2>Το trend στις αρχιτεκτονικές νευρωνικών δικτύων</h2>
                    <p>
                        Με το Deep Learning πολλά φράγματα απόδοσης ξεπεράστηκαν. Κάθε χρόνο βγαίνει ένα νέο paper που προτείνει μια αρχιτεκτονική που 
                        ξεπερνάει σε απόδοση τις προηγούμενες. Η τρέχουσα state-of-the-art αρχιτεκτονική για το ImageNet είναι ένα νευρωνικό δίκτυο
                        με 480 εκαττομύρια παραμέτρους. Επίσης το ίδιο συμβαίνει στον χώρο της επεξεργασίας φυσικής γλώσσας. Μέσα στα τελευταία δύο χρόνια
                        η OpenAI δημοσίευσε 3 papers το οποίο παρουσίαζε 3 αρχιτεκτονικές GPT-1, GPT-2, GPT-3, με την κάθε μία να πετυχαίνει state-of-the-art
                        αποτελέσματα. Η τελευταία αρχιτεκτονική μάλιστα που δημοσιεύτηκε το 2020 περιέχει 175 δισεκατομμύρια παραμέτρους, και το υπολογιστικό
                        κόστος για να εκπαιδευτεί έφτασε τα 4,6 εκατομμύρια δολλάρια.
                    </p>
                </section>

                <section>
                    <h2>Συμπεράσματα</h2>
                    <p>
                        Το bottleneck τώρα πιά δεν είναι ποιά αρχιτεκτονική να διαλέξουμε· Είναι η υπολογιστική ισχύς και τα δεδομένα που υπάρχουν.
                    </p>
                    <p>
                        Αν είχαμε θεωρητικά άπλετη υπολογιστική ισχύ και ταχύτητα, ο κάθε ερευνητής θα μπορούσε να εκπαιδεύσει το δικό του GPT-3 για 
                        οποιοδήποτε πρόβλημα ήθελε. Τόσο βαθιές αρχιτεκτονικές κάνουν δύσκολη την εφαρμογή σε μικρότερα και ποιό άγνωστα προβλήματα.
                    </p>
                    <p>
                        Επίσης, τα δεδομένα που υπάρχουν είναι άλλο ένα bottleneck. Αν είχαμε άπειρα δεδομένα εκπαίδευσης θα μπορούσαμε να αυτοματοποιήσουμε
                        θεωρητικά τα πάντα
                    </p>
                </section>
            </article>
            

        </main>
        <aside>
            <h2>Χρήσιμο υλικό για το Deep Learning</h2>
            <a href="https://www.fast.ai/">Fast AI - Deep Learning library and course</a>
            <a href="http://nlpprogress.com/">NLPProgress - A site tracking the state of the art in natural language processing</a>
        </aside>
        <footer>
            <p>Copyright 2021 - N.Smyrnioudis</p>
        </footer>
    </body>
</html>