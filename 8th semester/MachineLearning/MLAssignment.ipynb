{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from math import sqrt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     8,
     10,
     14,
     19,
     22,
     24
    ]
   },
   "outputs": [],
   "source": [
    "# model , functions\n",
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )\n",
    "\n",
    "def logexp(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "def logexp_d(x):\n",
    "    a = np.exp(x)\n",
    "    return a / (1 + a)\n",
    "\n",
    "def tanh(x):\n",
    "    a = np.exp(x)\n",
    "    b = np.exp(-1 * x)\n",
    "    return (a - b ) / (a + b)\n",
    "\n",
    "def tanh_d(x):\n",
    "    return 1 - (tanh(x) ** 2 )\n",
    "\n",
    "def cos(x):\n",
    "    return np.cos(x)\n",
    "def cos_d(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "def identity_d(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "class Model:\n",
    "    def __init__(self , l2_reg , n_classes , hidden_size\n",
    "                 ,n_inputs , activation=\"logexp\" ):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # We add a bias to n_inputs so D needs to be == n_inputs\n",
    "        self.K = n_classes\n",
    "        self.M = hidden_size\n",
    "        self.D = n_inputs\n",
    "        w1_fan_in = self.D+1\n",
    "        w2_fan_in = self.M+1\n",
    "        # TODO proper init\n",
    "        self.W2 = (np.random.rand(self.K , self.M+1) * 2 - 1) * 1 / sqrt(w2_fan_in) \n",
    "        self.W1 = (np.random.rand(self.M , self.D+1) * 2  - 1) * 1 / sqrt(w1_fan_in)\n",
    "        \n",
    "        if activation==\"logexp\":\n",
    "            self.activation=logexp\n",
    "            self.activation_d=logexp_d\n",
    "        elif activation==\"tanh\":\n",
    "            self.activation=tanh\n",
    "            self.activation_d=tanh_d\n",
    "        elif activation==\"cos\":\n",
    "            self.activation=cos\n",
    "            self.activation_d=cos_d\n",
    "        else:\n",
    "            self.activation=identity\n",
    "            self.activation_d=identity_d\n",
    "            print(\"Invalid activation provided, using identity\")\n",
    "            \n",
    "        self.l2 = l2_reg\n",
    "        self.h = self.activation\n",
    "    def parameters(self):\n",
    "        return [self.W1 , self.W2]\n",
    "    def set_parameters(self, params):\n",
    "        self.W1 = params[0]\n",
    "        self.W2 = params[1]\n",
    "    \n",
    "    def __call__(self, _input):\n",
    "        \"\"\"\n",
    "        Does the models forward pass\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def score(self, _input , _target):\n",
    "        \"\"\"\n",
    "        Returns the loss of the model at _input with target _target.\n",
    "        \"\"\"\n",
    "        # input (Nb, D + 1)\n",
    "        # target (Nb, K)\n",
    "        Nb = _input.shape[0]\n",
    "        # Add ones for bias\n",
    "        _input = np.concatenate((np.ones((Nb,1), dtype=np.float), _input), axis=-1)\n",
    "        zetas = self.h(_input @ self.W1.T) # (Nb, M)\n",
    "        # Add ones for bias\n",
    "        zetas = np.concatenate((np.ones((Nb,1), dtype=np.float), zetas) , axis=-1) # (Nb, M+1)\n",
    "        logits = zetas @ self.W2.T\n",
    "        probabilities = softmax(logits)\n",
    "        \n",
    "        # Saving these arrays for gradient computation\n",
    "        self.T = _target\n",
    "        self.X = _input\n",
    "        self.Y = probabilities\n",
    "        self.Z = zetas\n",
    "        \n",
    "        log_likelihood =  np.sum(_target * np.log(probabilities))\n",
    "        reg_term = - (self.l2 /2) * (np.sum(self.W1 * self.W1) + np.sum(self.W2 * self.W2))\n",
    "        \n",
    "        loss = log_likelihood + reg_term\n",
    "        return loss\n",
    "    def predict(self, _input):\n",
    "        \"\"\"\n",
    "        Returns the predicted classes \n",
    "        \"\"\"\n",
    "        # input (Nb, D + 1)\n",
    "        # target (Nb, K)\n",
    "        Nb = _input.shape[0]\n",
    "        _input = np.concatenate((np.ones((Nb,1), dtype=np.float), _input), axis=-1)\n",
    "        zetas = self.h(_input @ self.W1.T) # (Nb, M)\n",
    "        zetas = np.concatenate((np.ones((Nb,1), dtype=np.float), zetas) , axis=-1) # (Nb, M+1)\n",
    "        logits = zetas @ self.W2.T\n",
    "        probabilities : np.ndarray = softmax(logits)\n",
    "        return probabilities.argmax(axis=-1)\n",
    "        \n",
    "        \n",
    "    def grads(self):\n",
    "        \"\"\"\n",
    "        REturns the gradients for the model's parameters.\n",
    "        The output of this method and the output of the @parameters\n",
    "        method should be both listswith the same length and each \n",
    "        element must be a numpy array with the same shapes  \n",
    "        \"\"\"\n",
    "        TminusY = self.T - self.Y\n",
    "        gradW2 = TminusY.T @ self.Z  - (self.l2 * self.W2)\n",
    "        A = self.activation_d(self.X @ self.W1.T)\n",
    "        #gradW1 = (((TminusY @ self.W2[:,1:]).T * A.T) @ self.X) \n",
    "        gradW1 = (((TminusY @ self.W2[:,1:]).T) * A.T @ self.X)- (self.l2 * self.W1)\n",
    "        return [gradW1, gradW2]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa873ec97254f24aed4f14cec6828f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f4d6d2ff3d4bbcac89a3dc70d22992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient checks passed!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def gradcheck(model, tol=10e-4, epsilon=10e-6, batch_size=150):\n",
    "    \"\"\"\n",
    "    Performs a numerical gradient check of the model \n",
    "    \"\"\"\n",
    "    test_input = np.random.rand(batch_size, model.n_inputs)\n",
    "    test_target = np.zeros((batch_size,model.n_classes))\n",
    "    # Set each examples class randomly\n",
    "    for i in range(0,batch_size):\n",
    "        test_target[i][random.randrange(0,model.n_classes)] =1 \n",
    "\n",
    "    test_loss = model.score(test_input, test_target)\n",
    "    test_grad = model.grads()\n",
    "    # Perform gradient check\n",
    "    count = 0\n",
    "    all_ok = True\n",
    "    \n",
    "    for param , grad in zip(model.parameters(), test_grad):\n",
    "        it = np.nditer(param, flags=['multi_index'] , op_flags=['writeonly'])\n",
    "        for x in tqdm(it):\n",
    "            previous_value = x\n",
    "            target_grad = grad[it.multi_index]\n",
    "            \n",
    "            # Evalutaing at w + e\n",
    "            param[it.multi_index] = x + epsilon\n",
    "            wpe = model.score(test_input, test_target)\n",
    "            \n",
    "            # Evaluating at w - e\n",
    "            param[it.multi_index] = x - 2*epsilon\n",
    "            wme = model.score(test_input, test_target)\n",
    "            \n",
    "            # Resetting the model param\n",
    "            param[it.multi_index] = previous_value\n",
    "            \n",
    "            num_grad = (wpe - wme) / (2 * epsilon)\n",
    "            if abs(num_grad - target_grad) > tol:\n",
    "                all_ok = False\n",
    "                print(\"num grad \" , num_grad)\n",
    "                print(\"target grad \" , target_grad)\n",
    "                diff = abs(num_grad - target_grad)\n",
    "                print(\"GRADIENT CHECK ERROR \" + \n",
    "                      \"%.2f paramorder %d coordinate %s dif %.2f\" % (x, count ,it.multi_index, diff))\n",
    "        count += 1\n",
    "                \n",
    "    if all_ok:\n",
    "        print(\"Gradient checks passed!\")\n",
    "    else:\n",
    "        print(\"Gradient checks failed\")\n",
    "model = Model(0.01 , n_classes=8, hidden_size=10 , n_inputs=5, activation=\"logexp\")\n",
    "gradcheck(model, batch_size=3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "class MnistDataset():\n",
    "    \n",
    "    def _parse_mnist_file(self, filepath, label):\n",
    "        x= None\n",
    "        y= None\n",
    "        with open(filepath , 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = \" \".join(lines)\n",
    "            x= np.fromstring(lines, dtype=int, sep=' ').reshape(-1 , 28 * 28)\n",
    "            y = np.zeros((x.shape[0], 10))\n",
    "            y[: , label] = 1\n",
    "            \n",
    "        return x , y\n",
    "    def train(self):\n",
    "        return self.train_x , self.train_y\n",
    "    \n",
    "    def dev(self):\n",
    "        return self.dev_x , self.dev_y\n",
    "    \n",
    "    def test(self):\n",
    "        return self.test_x , self.test_y\n",
    "        \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        loads the mnist dataset from the path\n",
    "        \"\"\"\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        \n",
    "        self.test_x = []\n",
    "        self.test_y = []\n",
    "        print(\"Loading MNIST dataset\")\n",
    "        s = os.listdir(path)\n",
    "        for filename in s:\n",
    "            curr_label = int(filename[-5])\n",
    "            x , y = self._parse_mnist_file(path + \"/\" + filename, curr_label)\n",
    "            if 'train' in filename:\n",
    "                self.train_x.append(x)\n",
    "                self.train_y.append(y)\n",
    "            elif 'test' in filename:\n",
    "                self.test_x.append(x)\n",
    "                self.test_y.append(y)\n",
    "            else:\n",
    "                raise Exception(f\"Unexpected mnist file {filename}\")\n",
    "        self.train_x = np.concatenate(self.train_x , axis=0) / 255\n",
    "        self.train_y = np.concatenate(self.train_y , axis=0)\n",
    "        \n",
    "        self.test_x = np.concatenate(self.test_x , axis=0) / 255\n",
    "        self.test_y = np.concatenate(self.test_y , axis=0)\n",
    "        \n",
    "        # Creating dev subsets\n",
    "        self.train_x, self.dev_x , self.train_y, self.dev_y \\\n",
    "            = train_test_split(self.train_x , self.train_y , test_size=0.3)\n",
    "\n",
    "import pickle\n",
    "from itertools import chain\n",
    "class CifarDataset():\n",
    "    def _parse_cifar_file(self, filename):\n",
    "        data = None\n",
    "        labels = None\n",
    "        with open(filename, 'rb') as f:\n",
    "            x = pickle.load(f, encoding='bytes')\n",
    "            labels = np.array(x[b'labels'])\n",
    "            data = x[b'data'].astype(np.float)\n",
    "            \n",
    "        return data, labels\n",
    "    \n",
    "    def train(self):\n",
    "        return self.train_x , self.train_y\n",
    "    \n",
    "    def dev(self):\n",
    "        return self.dev_x , self.dev_y\n",
    "    \n",
    "    def test(self):\n",
    "        return self.test_x , self.test_y\n",
    "    \n",
    "    def __init__(self, path ):\n",
    "        print(\"Loading CIFAR-10 dataset\")\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        \n",
    "        self.test_x = []\n",
    "        self.test_y = []\n",
    "        \n",
    "        s = os.listdir(path)\n",
    "        for filename in s:\n",
    "            if 'data_batch' in filename:\n",
    "                x , y = self._parse_cifar_file(path + \"/\" + filename)\n",
    "                self.train_x.append(x)\n",
    "                self.train_y.append(y)\n",
    "            elif 'test_batch' in filename:\n",
    "                x , y = self._parse_cifar_file(path + \"/\" + filename)\n",
    "                self.test_x.append(x)\n",
    "                self.test_y.append(y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.train_x = np.concatenate(self.train_x , axis=0) / 255\n",
    "        self.train_y = np.concatenate(self.train_y , axis=0)\n",
    "        \n",
    "        self.test_x = np.concatenate(self.test_x , axis=0) / 255\n",
    "        self.test_y = np.concatenate(self.test_y , axis=0)\n",
    "        \n",
    "        # Need to convert labels to the target array ( one hot vectors)\n",
    "        max_label = max(self.train_y.max() , self.train_y.max())\n",
    "        one_hot_vectors = np.eye(max_label + 1)\n",
    "        \n",
    "        self.train_y = one_hot_vectors[self.train_y]\n",
    "        self.test_y = one_hot_vectors[self.test_y]\n",
    "        \n",
    "        # Creating dev subsets\n",
    "        self.train_x, self.dev_x , self.train_y, self.dev_y \\\n",
    "            = train_test_split(self.train_x , self.train_y , test_size=0.3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#iteration methods\n",
    "def train(model , x, y, batch_size , learning_rate, n_epochs , with_tqdm=True):\n",
    "    # Batched stochastic gradient ascent\n",
    "    pbar = range(n_epochs)\n",
    "    if with_tqdm:\n",
    "        pbar = tqdm(pbar)\n",
    "    for i in pbar:\n",
    "        for xb, yb in BatchSampler(x ,y, batch_size , with_tqdm=False):\n",
    "            loss = model.score(xb,yb)\n",
    "            params = model.parameters()\n",
    "            grads = model.grads()\n",
    "            new_params = [None] * len(params)\n",
    "            for i in range(len(params)):\n",
    "                new_params[i] = params[i] + (learning_rate* grads[i])\n",
    "            model.set_parameters(new_params)\n",
    "            if with_tqdm:\n",
    "                pbar.set_postfix_str(\"loss: %.4f\" % (loss))\n",
    "    \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, x , y , with_tqdm=True):\n",
    "    loss_sum = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for xb, yb in BatchSampler(x ,y, 200 , with_tqdm=with_tqdm):\n",
    "        curr_y_true = yb.argmax(axis=-1)\n",
    "        curr_y_pred = model.predict(xb)\n",
    "        \n",
    "        y_true.append(curr_y_true)\n",
    "        y_pred.append(curr_y_pred)\n",
    "        \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    return   1 - accuracy_score(y_true , y_pred)\n",
    "\n",
    "def BatchSampler(x , y , batch_size, with_tqdm=True):\n",
    "    num_examples = x.shape[0]\n",
    "    generator = range(0, num_examples, batch_size)\n",
    "    \n",
    "    if with_tqdm:\n",
    "        generator = tqdm(generator)\n",
    "        \n",
    "    for start_idx in generator:\n",
    "        end_idx = min(num_examples-1 , (start_idx + batch_size) -1 )\n",
    "        yield x[start_idx:end_idx,:] , y[start_idx:end_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    loaded_dsets = {}\n",
    "    def get_dataset(self, ds):\n",
    "        if ds==\"mnist\":\n",
    "            if ds not in DatasetLoader.loaded_dsets:\n",
    "                DatasetLoader.loaded_dsets['mnist'] = MnistDataset(\"./data/mnist/\")\n",
    "            return DatasetLoader.loaded_dsets['mnist']\n",
    "            \n",
    "        elif ds==\"cifar\":\n",
    "            if ds not in DatasetLoader.loaded_dsets:\n",
    "                DatasetLoader.loaded_dsets['cifar'] = CifarDataset(\"./data/cifar/\")\n",
    "            return DatasetLoader.loaded_dsets['cifar']\n",
    "        else:\n",
    "            raise Exception(\"invalid dataset provided\")\n",
    "DatasetLoader.get_dataset = classmethod(DatasetLoader.get_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split            \n",
    "class Scenario:\n",
    "    def __init__(self, lr , n_epochs , batch_size , l2_reg, hidden_size \n",
    "                 , dataset , activation , execute_on_dev, verbose):\n",
    "        self._dict = {}\n",
    "        self._dict['learning_rate'] = lr\n",
    "        self._dict['n_epochs'] = n_epochs\n",
    "        self._dict['batch_size'] = batch_size\n",
    "        self._dict['hidden_size'] = hidden_size\n",
    "        self._dict['l2_reg'] = l2_reg\n",
    "        self._dict['dataset'] = dataset\n",
    "        self._dict['activation'] = activation\n",
    "        self._dict['execute_on_dev'] = execute_on_dev\n",
    "        self._dict['verbose'] = verbose\n",
    "        \n",
    "    def execute(self):\n",
    "        dataset_str = self['dataset']\n",
    "        dataset = DatasetLoader.get_dataset(dataset_str)\n",
    "        \n",
    "            \n",
    "        n_classes  = dataset.test()[1].shape[1]\n",
    "        n_inputs  = dataset.test()[0].shape[1] \n",
    "        xt , yt = dataset.train()\n",
    "        self.model = Model(self['l2_reg'] , n_classes, \n",
    "                           self['hidden_size'] , n_inputs , self['activation'])\n",
    "        train(self.model , xt, yt ,self['batch_size'] ,\\\n",
    "              self['learning_rate'], self['n_epochs'] ,with_tqdm=self['verbose'] )\n",
    "        \n",
    "        \n",
    "        xval , yval = dataset.dev() if self['execute_on_dev'] else dataset.test()\n",
    "        error = evaluate(self.model , xval , yval, with_tqdm=self['verbose'])\n",
    "        if self['verbose']:\n",
    "            print(\"error is %.2f %%\" % (error * 100))\n",
    "        return error\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self._dict)\n",
    "    def __getitem__(self, x):\n",
    "        return self._dict[x]\n",
    "a = Scenario(\n",
    "             lr=0.001,\n",
    "             n_epochs=1,\n",
    "             batch_size=150,\n",
    "             hidden_size=500,\n",
    "             l2_reg=0.00001,\n",
    "             dataset=\"cifar\",\n",
    "             activation=\"cos\",\n",
    "             execute_on_dev=True,\n",
    "             verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.001, 'n_epochs': 1, 'batch_size': 150, 'hidden_size': 500, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': True}\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def Tune():\n",
    "    lrs = [0.01 , 0.001 , 0.0001 ]\n",
    "    n_epochs = [ 0 ]\n",
    "    batch_sizes = [150]\n",
    "    hidden_sizes=[100 , 200 , 300]\n",
    "    l2_regs = [0.0001 , 0.00001 , 0.000001 ]\n",
    "    datasets=  [\"mnist\" , \"cifar\"]\n",
    "    activations= [\"logexp\" , \"tanh\" , \"cos\"]\n",
    "    execute_on_dev=[True]\n",
    "    verbose=[False]\n",
    "    generator = tqdm(\n",
    "        list(product(\n",
    "            lrs , n_epochs , batch_sizes,l2_regs, hidden_sizes,\n",
    "            datasets , activations, execute_on_dev , verbose)))\n",
    "    results = []\n",
    "    for x in generator:\n",
    "        a = Scenario(*x)\n",
    "        result = a.execute()\n",
    "        results.append([result , a])\n",
    "        \n",
    "    df = pd.DataFrame(data=results)\n",
    "    df.to_csv(\"tune_res\")\n",
    "        \n",
    "    print(\"min scenario error \" , min(results, key=lambda x : x[0])[0])\n",
    "    print(\"min error scenario \" , min(results, key=lambda x : x[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1ac21d88a5412c9b85c30c5814350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "min scenario error  0.7989949748743719\n",
      "min error scenario  {'learning_rate': 0.01, 'n_epochs': 0, 'batch_size': 150, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "Tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"reeltune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_epochs': 50, 'batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.070128</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_epochs': 50, 'batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.760469</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_epochs': 50, 'batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.899162</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_epochs': 50, 'batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.903317</td>\n",
       "      <td>{'learning_rate': 0.01, 'n_epochs': 50, 'batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157</td>\n",
       "      <td>0.065662</td>\n",
       "      <td>{'learning_rate': 0.0001, 'n_epochs': 50, 'bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158</td>\n",
       "      <td>0.046845</td>\n",
       "      <td>{'learning_rate': 0.0001, 'n_epochs': 50, 'bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159</td>\n",
       "      <td>0.552697</td>\n",
       "      <td>{'learning_rate': 0.0001, 'n_epochs': 50, 'bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160</td>\n",
       "      <td>0.539363</td>\n",
       "      <td>{'learning_rate': 0.0001, 'n_epochs': 50, 'bat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>161</td>\n",
       "      <td>0.507136</td>\n",
       "      <td>{'learning_rate': 0.0001, 'n_epochs': 50, 'bat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0         0                                                  1\n",
       "0             0  0.044779  {'learning_rate': 0.01, 'n_epochs': 50, 'batch...\n",
       "1             1  0.070128  {'learning_rate': 0.01, 'n_epochs': 50, 'batch...\n",
       "2             2  0.760469  {'learning_rate': 0.01, 'n_epochs': 50, 'batch...\n",
       "3             3  0.899162  {'learning_rate': 0.01, 'n_epochs': 50, 'batch...\n",
       "4             4  0.903317  {'learning_rate': 0.01, 'n_epochs': 50, 'batch...\n",
       "..          ...       ...                                                ...\n",
       "157         157  0.065662  {'learning_rate': 0.0001, 'n_epochs': 50, 'bat...\n",
       "158         158  0.046845  {'learning_rate': 0.0001, 'n_epochs': 50, 'bat...\n",
       "159         159  0.552697  {'learning_rate': 0.0001, 'n_epochs': 50, 'bat...\n",
       "160         160  0.539363  {'learning_rate': 0.0001, 'n_epochs': 50, 'bat...\n",
       "161         161  0.507136  {'learning_rate': 0.0001, 'n_epochs': 50, 'bat...\n",
       "\n",
       "[162 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.columns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = a[a.columns[1]].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = a[a.columns[2]].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sands = [[x , y] for x , y in zip(scores , scenarios)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sands = sorted(sands , key= lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.021440536013400343,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.022222222222222254,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.023059743160245727,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02322724734785031,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02322724734785031,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02333891680625344,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02350642099385813,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.0235622557230597,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02378559463986596,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.024288107202680043,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.024678950307091,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.024958123953098817,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.025013958682300386,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02540480178671134,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02540480178671134,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.025460636515912904,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02551647124511447,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02680067001675046,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.027414852037967563,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.027861529871580082,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.028029034059184776,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.028587381351200425,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.028587381351200425,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.028643216080401986,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.02881072026800668,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.029145728643216073,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.029145728643216073,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.033780011166945845,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.03584589614740363,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.0389168062534897,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04103852596314905,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04165270798436626,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.042936906756002265,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.043439419318816334,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.043606923506421036,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04405360134003356,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.044165270798436566,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04477945281965379,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.044891122278056916,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.045114461194863176,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.045561139028475695,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04628699050809604,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.046845337800111686,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.046957007258514816,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.04807370184254611,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.048185371300949235,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.049916247906197635,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05058626465661642,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.051367950865438335,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05438302624232272,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05695142378559459,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.058403126744835276,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05851479620323841,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05924064768285875,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.05929648241206033,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06180904522613063,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06192071468453375,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06292573981016192,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06566164154103847,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06566164154103847,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06616415410385257,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.06890005583472925,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07012841987716356,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07470686767169178,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07487437185929646,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07543271915131211,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07587939698492463,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07677275265214968,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07699609156895593,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07733109994416533,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07755443886097158,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.07833612506979337,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5053266331658292,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.505929648241206,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5070016750418761,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5071356783919598,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5078056951423786,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.509212730318258,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5098827470686766,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5107537688442211,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5132328308207705,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.518391959798995,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5190619765494138,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5252261306532664,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5260971524288107,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5310552763819095,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5348073701842546,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5352763819095477,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5353433835845895,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5361474036850922,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.536214405360134,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5370184254606365,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5370854271356784,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.538358458961474,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5393634840871022,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5395644891122278,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5395644891122278,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5401005025125628,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5411725293132328,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5458626465661642,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5489447236180904,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5490117252931324,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5500837520938023,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5501507537688441,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.551356783919598,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5514237855946399,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5514907872696817,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5526968174204355,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5529648241206031,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5539698492462312,\n",
       "  \"{'learning_rate': 0.0001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5580569514237856,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.56107202680067,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5695142378559463,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.5984589614740369,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.6834170854271358,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.6868341708542713,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.688107202680067,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7212171970965942,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7291457286432161,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7348408710217755,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7431044109436069,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7485203796761586,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7559463986599665,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7604690117252931,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7615857063093244,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.7924064768285874,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'mnist', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8471691792294808,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8481742043551088,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8493132328308208,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8537353433835846,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8659966499162479,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8733668341708543,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8808710217755444,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8818090452261307,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8820770519262982,\n",
       "  \"{'learning_rate': 0.001, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8966834170854271,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8970854271356784,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990284757118928,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990284757118928,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990954773869346,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990954773869346,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990954773869346,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8990954773869346,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8991624790619765,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8991624790619765,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8991624790619765,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8992294807370185,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8992294807370185,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'logexp', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.8993634840871022,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9006365159128978,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9024455611390284,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 0.0001, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 100, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9033165829145728,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 300, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'tanh', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.903852596314908,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-06, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"],\n",
       " [0.9044556113902849,\n",
       "  \"{'learning_rate': 0.01, 'n_epochs': 50, 'batch_size': 200, 'hidden_size': 200, 'l2_reg': 1e-05, 'dataset': 'cifar', 'activation': 'cos', 'execute_on_dev': True, 'verbose': False}\"]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bdc1e61ab5496ca2b95787eea6c3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e537d6e2a40949546d6092798f188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "error is 50.40 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5040201005025126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MNIST run\n",
    "a = Scenario(\n",
    "             lr=0.0001,\n",
    "             n_epochs=50,\n",
    "             batch_size=200,\n",
    "             hidden_size=300,\n",
    "             l2_reg=1e-05,\n",
    "             dataset=\"cifar\",\n",
    "             activation=\"cos\",\n",
    "             execute_on_dev=False,\n",
    "             verbose=True\n",
    ")\n",
    "a.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
